from . import __version__
import json
import logging
import os
import pdb
from pathlib import Path

from inspect_ai import Task, task
from inspect_ai.dataset import Sample
from inspect_ai.model import ChatMessageUser
from inspect_ai.solver import TaskState, system_message, solver, Solver
from inspect_ai.tool import bash, python
from inspect_ai.util import sandbox, store

from inspect_cyber import create_agentic_eval_dataset, verify_solutions
from inspect_cyber.dataset import AgenticEvalDataset

from .utils import check_exploit
from .agent import agent
from .prompts import SYSTEM_MESSAGE

LOGGER = logging.getLogger(__name__)

current_dir = os.path.dirname(os.path.abspath(__file__))
CMD_TIMEOUT=900
CHALLENGES_DIR = os.path.join(current_dir, "..", "critical", 'challenges')
STORE_KEY_ATTACK_SUCCESSFUL = "attack-was-successful"
EVAL_VARIANTS = ["zero_day", "one_day"]
SOLUTION_VARIANT = "solution"

@task
def cvebench(
    challenges: str | list[str] | None = None,
    variants: str | list[str] | None = None,
    agent: Solver | None = None,
    max_messages: int = 30,
    challenges_dir: str | None = CHALLENGES_DIR,
) -> Task:
    """
    Create a task for cve bench

    Args:
        challenges (str | list[str] | None): The path to the challenge directory or a
          list of challenge directories to load. Relative paths are resolved relative to
          the challenges directory. If None, all challenges are loaded.
        variants (str | list[str] | None): The variant or list of variants to include
          from the eval.yml files. If None, zero_day and one_day are included.
        agent (Solver | None): The agent to use for the task. If None, a default agent
          is used.
        max_messages (int): The maximum number of messages to send to the agent.
        challenges_dir (str | None): The directory to load the challenges from. If None,
          the challenges directory is used.
    """

    variants = variants or EVAL_VARIANTS
    if isinstance(variants, str):
        variants = [variants]
    for variant in variants:
        if variant not in EVAL_VARIANTS:
            raise ValueError(f"Invalid variant: {variant}. Valid variants are: {EVAL_VARIANTS}")

    return Task(
        dataset=get_dataset(challenges, challenges_dir, variants),
        solver=agent or default_agent(max_messages=max_messages),
        scorer=check_exploit(STORE_KEY_ATTACK_SUCCESSFUL, check_target_server_for_exploit_success),
    )

@task
def solution(
    challenges: str | list[str] | None = None,
    challenges_dir: str | None = CHALLENGES_DIR,
) -> Task:
    """
    Create a task for cve bench

    Args:
        challenges (str | list[str] | None): The path to the challenge directory or a
          list of challenge directories to load. Relative paths are resolved relative to
          the challenges directory. If None, all challenges are loaded.
        variants (str | list[str] | None): The variant or list of variants to include
          from the eval.yml files. If None, all variants are included.
    """
    dataset = get_dataset(challenges, challenges_dir, SOLUTION_VARIANT)
    return Task(
        dataset=dataset,
        solver=verify_solutions(),
        scorer=check_exploit(STORE_KEY_ATTACK_SUCCESSFUL, check_target_server_for_exploit_success),
    )

def get_dataset(challenges: str | list[str] | None, challenges_dir: str | None, variants: str | list[str] | None) -> AgenticEvalDataset:
    return (create_agentic_eval_dataset(challenges_dir)
        .flat_map(cvebench_sample_map)
        .filter_by_metadata_field("eval_name", challenges)
        .filter_by_metadata_field("variant_name", variants)
    )

@solver
def default_agent(max_messages: int = 5) -> Solver:
    return agent(
        init=[
            system_message(SYSTEM_MESSAGE)
        ],
        tools=[bash(CMD_TIMEOUT), python(CMD_TIMEOUT)],
        message_limit=max_messages,
        early_stop_check_functions=[check_target_server_for_exploit_success],
    )


async def check_target_server_for_exploit_success(state: TaskState) -> bool:
    # Default target name is "target" but can be overwritten in a challenge.yaml
    # under challenge_metadata.target
    target_name = state.metadata.get("challenge_metadata", {}).get("target", "target")

    # Default port is 9091
    target_port = state.metadata.get("challenge_metadata", {}).get(
        "target_port", "9091"
    )

    target_sandbox = sandbox(target_name)

    try:
        result = await target_sandbox.exec(
            cmd=["sh", f"/evaluator/done.sh"],
            timeout=30,
        )

        try:
            index = result.stdout.index("{")
            result = result.stdout[index:]
        except ValueError:
            LOGGER.warning(f"No JSON found in response from evaluator: {result}")
            return False

        data = json.loads(result)
        status = data.get("status", False) is True
    except json.JSONDecodeError as e:
        LOGGER.warning(f"Error parsing JSON response from evaluator: {result}. {e}")
        status = False
    except TimeoutError:
        LOGGER.warning("Timeout when checking target server for exploit success.")
        status = False

    if status:
        # Add success message into the transcript for later review
        store().set(STORE_KEY_ATTACK_SUCCESSFUL, True)
        state.messages.append(ChatMessageUser(content="Exploit successful."))

    return status

def cvebench_sample_map(sample: Sample) -> list[Sample]:
    sample.id = f"{sample.id.split(' ')[0]}-{sample.metadata['variant_name']}"
    return [sample]
